---
title: "random-forest"
author: "Rohan Krishnan"
date: "2024-07-30"
output: html_document
---
# Random Forest (with tuning)

## Load libraries + functions
```{r, message = FALSE}
source("../scripts/useful_functions.R")
library(tidyverse)
library(randomForest)
library(caret)
library(gridExtra)
```

## Load data + split
```{r}
#Load and preprocess
df <- read.csv("../data/modelling/modelling.csv")
model_df_preprocess(df)

#Split
train_test_split(df, 0.70, 0.30)
```

## Random Forest (untuned)

###Create inital random forest model
```{r}
#Fit model
set.seed(100)
model_forest <- randomForest(clcsHPI ~ ., data = train, ntree = 1000, importance = TRUE)
```

### Look at important variables
```{r}
forest_imp_vars <- as.data.frame(importance(model_forest))
forest_imp_vars$name <- row.names(forest_imp_vars)

plot_forest_imp_vars <- forest_imp_vars %>%
  arrange(`%IncMSE`, IncNodePurity) %>%
  mutate(Name = factor(name, levels = name)) %>%
  ggplot(aes(x = Name, y = `%IncMSE`)) + 
  geom_segment(aes(x = Name, xend = Name, y = 0, yend = `%IncMSE`), color = palette[1]) +
  geom_point(aes(size = IncNodePurity), color = palette[2], alpha = 0.8) + 
  labs(title = "Untuned Random Forest") + 
  theme_bw() + 
  coord_flip() + 
  theme(
    plot.title = element_text(face = "bold"),
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank(),
  )

plot_forest_imp_vars
```

### Errors
```{r}
#Predict
pred_forest <- predict(model_forest, test)

#MSE -- 2.273641 -- super low!
mean((pred_forest - test$clcsHPI)^2)

#RMSE-- 1.50786 -- very similar to MSE
sqrt(mean((pred_forest - test$clcsHPI)^2))

#MAE -- 1.107139
mean(abs(pred_forest - test$clcsHPI))

#MAPE -- 0.006166471
mean(abs((pred_forest - test$clcsHPI)/test$clcsHPI))
```

## Random Forest (tuned)

### Find best tuning parameters using tuneRF
```{r}
# Algorithm Tune (tuneRF)
set.seed(100)
mtry_best <- tuneRF(x = train[,c(1,2,4:17)], y = train[,3], stepFactor=1.5, improve=1e-5, ntree=501)
print(mtry_best)
plot(mtry_best) #mtry of 7 seems to work best
```

### Grid search for tuning parameters using caret
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")

set.seed(100)
tune_grid <- expand.grid(.mtry=c(1:8))
forest_grid_search <- train(clcsHPI~., data=train, 
                          method="rf", metric="RMSE", 
                          tuneGrid=tune_grid, trControl=control,)
print(forest_grid_search)
plot(forest_grid_search)
```

It looks like tuneRF and our grid search disagree on the optimal mtry. I'll go with an mtry of 8 since it is close to tuneRF's value of 7 but seems to be slightly better for RMSE.  

### Fit tuned random forest
```{r}
#Fit tuned model using tuneRF
set.seed(100)
model_forest_tuned <- randomForest(clcsHPI~., data=train, 
                           ntree=1000, mtry=8, importance = TRUE)
```

### Look at important variables
```{r}
forest_tuned_imp_vars <- as.data.frame(importance(model_forest_tuned))
forest_tuned_imp_vars$names <- row.names(forest_tuned_imp_vars)

plot_forest_tuned_imp_vars <- forest_tuned_imp_vars %>%
  arrange(`%IncMSE`, IncNodePurity) %>%
  mutate(Name = factor(names, levels = names)) %>%
  ggplot(aes(x = Name, y = `%IncMSE`)) + 
  geom_segment(aes(x = Name, xend = Name, y = 0, yend = `%IncMSE`), color = palette[1]) +
  geom_point(aes(size = IncNodePurity), color = palette[2], alpha = 0.8) + 
  labs(title = "Tuned Random Forest") + 
  theme_bw() + 
  coord_flip() + 
  theme(
    plot.title = element_text(face = "bold"),
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank(),
  ); plot_forest_tuned_imp_vars
```

## Errors
```{r}
#Predict
pred_forest_tuned <- predict(model_forest_tuned, test)

#MSE -- 2.228529 -- slightly better than untuned
mean((pred_forest_tuned - test$clcsHPI)^2)

#RMSE -- 1.492826
sqrt(mean((pred_forest_tuned - test$clcsHPI)^2))

#MAE -- 1.09951
mean(abs(pred_forest_tuned - test$clcsHPI))

#MAPE -- 0.006213133
mean(abs((pred_forest_tuned - test$clcsHPI)/test$clcsHPI))
```

## Compare variable importance between tuned and untuned forest
```{r}
rf_importance_plots <- grid.arrange(plot_forest_imp_vars, plot_forest_tuned_imp_vars, nrow =1)

ggsave("rf-variable-imps.png",
       plot = rf_importance_plots,
       path = "../assets/",
       width = 14.5,
       height = 7.5,
       units = "in")
```


